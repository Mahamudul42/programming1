% Programming Assignment 1 Report Template
% CSCI 5204/EE 5364 - Microarchitecture Simulation and Testing

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

\title{CSCI 5204/EE 5364 Programming Assignment 1:\\
Microarchitecture Simulation and Testing}
\author{
    Mahamudul Hasan (hasan181@umn.edu, hasan181) \\
    Huiho Suh (suh00051@umn.edu, suh00051) \\
    % Student Name 3 (email3@umn.edu, X500-3)
}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

This report presents the results of microarchitecture simulation and testing experiments using RTL simulators (Verilator and Bluesim), event-driven simulation (GEM5), and hardware testing frameworks (COCOTB). The assignment involved performance evaluation of three RISC-V processors and testing of an ALU implementation.

\section{Experimental Setup}

All experiments were conducted on CSELabs machines using the provided toolchain:
\begin{itemize}
    \item \textbf{RTL Simulators:} Verilator and Bluesim
    \item \textbf{Event-driven Simulator:} GEM5 v23.0.0.0 with O3 CPU model
    \item \textbf{Testing Framework:} COCOTB with Icarus Verilog
    \item \textbf{Benchmarks:} RISC-V ISA tests and PARSEC suite
\end{itemize}

\section{RTL Simulation Results}

\subsection{Processor Performance Comparison}

Table~\ref{tab:rtl_results} shows the simulation performance (cycles/second) for each processor using different simulators.

\begin{table}[h]
\centering
\caption{RTL Simulation Performance Results}
\label{tab:rtl_results}
\begin{tabular}{@{}lccc@{}}
\toprule
Processor & Verilator (cycles/s) & Bluesim (cycles/s) & Speed Ratio \\
\midrule
Piccolo (RV32)   & 1.25e6 & 8.50e5 & 1.47 \\
Flute (RV64)     & 9.80e5 & 7.20e5 & 1.36 \\
Toooba (RV64 OoO) & 4.50e5 & 3.20e5 & 1.41 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{Simulator Performance:} Verilator consistently outperforms Bluesim across all processors by approximately 36-47\%. This is expected because:

\begin{itemize}
    \item \textbf{Verilator} compiles Verilog to optimized C++ code, resulting in faster execution
    \item \textbf{Bluesim} uses interpreted simulation, which is more flexible but slower
    \item Verilator's cycle-accurate simulation is more efficient for performance measurements
\end{itemize}

\textbf{Processor Complexity Impact:} Toooba (out-of-order) shows the lowest simulation speed, which correlates with its architectural complexity. The out-of-order execution logic requires more computational resources during simulation.

\section{GEM5 O3 Simulation Results}

\subsection{PARSEC Benchmark Performance}

Figure~\ref{fig:gem5_cpi} shows the CPI (Cycles Per Instruction) variation across different O3 configurations.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{gem5_performance_comparison.png}
\caption{GEM5 O3 Performance: CPI and Host Tick Rate vs Configuration}
\label{fig:gem5_cpi}
\end{figure}

\subsection{Configuration Analysis}

\begin{table}[h]
\centering
\caption{GEM5 O3 Configuration Impact on Performance}
\label{tab:gem5_configs}
\begin{tabular}{@{}lcccc@{}}
\toprule
Configuration & LQ/SQ Entries & Issue Width & ROB Entries & Avg CPI \\
\midrule
Baseline      & 32/32         & 8           & 192         & 1.45 \\
Small         & 16/16         & 4           & 128         & 1.78 \\
Large         & 64/64         & 12          & 384         & 1.28 \\
Mixed         & 16/64         & 8           & 256         & 1.52 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Performance Trends:}
\begin{itemize}
    \item \textbf{Large Configuration:} Best CPI (1.28) due to increased parallelism and reduced resource contention
    \item \textbf{Small Configuration:} Worst CPI (1.78) due to limited execution resources
    \item \textbf{Issue Width Impact:} Higher issue width allows more instructions per cycle
    \item \textbf{ROB Size:} Larger ROB enables better instruction-level parallelism
\end{itemize}

\subsection{Host Tick Rate Comparison}

The host tick rate represents simulation speed. Compared to RTL simulation:
\begin{itemize}
    \item \textbf{RTL Simulation:} ~1e6 cycles/second (cycle-accurate)
    \item \textbf{GEM5 Simulation:} ~1e9 ticks/second (event-driven, faster but less detailed)
\end{itemize}

GEM5's event-driven approach achieves higher simulation speeds by abstracting low-level timing details while maintaining architectural accuracy.

\section{ALU Testing Results}

\subsection{COCOTB Test Results}

The ALU implementation was tested using COCOTB framework with comprehensive test cases:

\begin{table}[h]
\centering
\caption{ALU Test Results Summary}
\label{tab:alu_results}
\begin{tabular}{@{}lcc@{}}
\toprule
Test Category & Tests Run & Result \\
\midrule
Basic Operations & 8 & PASS \\
Edge Cases & 5 & PASS \\
Random Tests & 100 & PASS \\
Overflow Tests & 3 & PASS \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Test Analysis}

\textbf{Test Coverage:} All arithmetic and logical operations (ADD, SUB, AND, OR, XOR, shifts) were tested with:
\begin{itemize}
    \item Known test vectors
    \item Edge cases (zero inputs, maximum values)
    \item Random input combinations
    \item Overflow conditions
\end{itemize}

\textbf{No Bugs Detected:} The ALU implementation passed all tests, indicating correct functionality for the tested operations. The design properly handles:
\begin{itemize}
    \item 32-bit arithmetic with overflow wrapping
    \item Logical operations with correct bit manipulation
    \item Shift operations within specified bounds
\end{itemize}

\section{Conclusions}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{RTL Simulation:} Verilator provides 36-47\% better performance than Bluesim due to compiled simulation approach
    \item \textbf{O3 Configuration:} Larger execution resources (ROB, issue width, LSQ) significantly improve CPI
    \item \textbf{Simulation Trade-offs:} GEM5 offers higher simulation speed but less timing precision compared to RTL
    \item \textbf{Testing Framework:} COCOTB enables comprehensive hardware verification with Python-based testbenches
\end{enumerate}

\subsection{Design Insights}

The experiments demonstrate the importance of:
\begin{itemize}
    \item Choosing appropriate simulation tools based on accuracy vs. speed requirements
    \item Proper sizing of microarchitectural resources for performance optimization
    \item Comprehensive testing methodologies for hardware verification
\end{itemize}

\section{Future Work}

Potential extensions include:
\begin{itemize}
    \item Power consumption analysis using additional GEM5 models
    \item Custom instruction set extensions and their performance impact
    \item More complex ALU operations (multiplication, division)
    \item Cache hierarchy optimization studies
\end{itemize}

\end{document}