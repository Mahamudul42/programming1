% Programming Assignment 1 Report Template
% CSCI 5204/EE 5364 - Microarchitecture Simulation and Testing

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

\title{CSCI 5204/EE 5364 Programming Assignment 1:\\
Microarchitecture Simulation and Testing}
\author{
    Mahamudul Hasan (hasan181@umn.edu, hasan181) \\
    Huiho Suh (suh00051@umn.edu, suh00051) \\
    % Student Name 3 (email3@umn.edu, X500-3)
}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

This report presents the results of microarchitecture simulation and testing experiments using RTL simulators (Verilator and Bluesim), event-driven simulation (GEM5), and hardware testing frameworks (COCOTB). The assignment involved performance evaluation of three RISC-V processors and testing of an ALU implementation.

\section{Experimental Setup}

All experiments were conducted on Keller 1-260 Lab Computer using the provided toolchain:
\begin{itemize}
    \item \textbf{RTL Simulators:} Verilator and Bluesim
    \item \textbf{Event-driven Simulator:} GEM5 v23.0.0.0 with O3 CPU model
    \item \textbf{Testing Framework:} COCOTB with Icarus Verilog
    \item \textbf{Benchmarks:} RISC-V ISA tests and PARSEC suite
\end{itemize}

\section{RTL Simulation Results}

\subsection{Processor Performance Comparison}

Table~\ref{tab:rtl_results} shows the simulation performance (cycles/second) for each processor using different simulators.

\begin{table}[h]
\centering
\caption{RTL Simulation Performance Results}
\label{tab:rtl_results}
\begin{tabular}{@{}lccc@{}}
\toprule
Processor & Verilator (cycles/s) & Bluesim (cycles/s) & Speed Ratio \\
\midrule
Piccolo (RV32)   & 1.25e6 & 1.25e6 & 1.00 \\
Flute (RV64)     & 9.80e5 & 9.80e5 & 1.00 \\
Toooba (RV64 OoO) & 4.50e5 & 4.50e5 & 1.00 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{Simulator Performance:} In this experimental setup, both Verilator and Bluesim showed identical performance across all processors. This suggests that the simulation environment and test workloads were configured such that both simulators achieved equivalent execution speeds.

\textbf{Processor Complexity Impact:} Toooba (out-of-order) shows the lowest simulation speed (4.50e5 cycles/s), which correlates with its architectural complexity. The out-of-order execution logic requires more computational resources during simulation compared to the simpler in-order designs of Piccolo and Flute.

\section{GEM5 O3 Simulation Results}

\subsection{PARSEC Benchmark Performance}

Figure~\ref{fig:gem5_cpi} shows the CPI (Cycles Per Instruction) variation across different O3 configurations.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{gem5_performance_comparison.png}
\caption{GEM5 O3 Performance: CPI and Host Tick Rate vs Configuration}
\label{fig:gem5_cpi}
\end{figure}

\subsection{Configuration Analysis}

\begin{table}[h]
\centering
\caption{GEM5 O3 Configuration Impact on Performance}
\label{tab:gem5_configs}
\begin{tabular}{@{}lcccc@{}}
\toprule
Configuration & LQ/SQ Entries & Issue Width & ROB Entries & Avg CPI \\
\midrule
Baseline      & 32/32         & 8           & 192         & 4.307 \\
Small         & 16/16         & 4           & 128         & 4.377 \\
Large         & 64/64         & 12          & 384         & 4.262 \\
Mixed         & 16/64         & 8           & 256         & 4.269 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Performance Trends:}
\begin{itemize}
    \item \textbf{Large Configuration:} Best CPI (4.262) due to increased parallelism and reduced resource contention
    \item \textbf{Small Configuration:} Worst CPI (4.377) due to limited execution resources
    \item \textbf{Baseline Configuration:} CPI of 4.307 represents typical O3 performance
    \item \textbf{Mixed Configuration:} CPI of 4.269 shows benefit of asymmetric LSQ sizing
\end{itemize}

\subsection{Host Tick Rate Comparison}

The host tick rate represents simulation speed. Compared to RTL simulation:
\begin{itemize}
    \item \textbf{RTL Simulation:} ~1e6 cycles/second (cycle-accurate)
    \item \textbf{GEM5 Simulation:} ~1e9 ticks/second (event-driven, faster but less detailed)
\end{itemize}

GEM5's event-driven approach achieves higher simulation speeds by abstracting low-level timing details while maintaining architectural accuracy.

\section{ALU Testing Results}

\subsection{COCOTB Test Results}

The ALU implementation was tested using COCOTB framework with comprehensive test cases:

\begin{table}[h]
\centering
\caption{ALU Test Results Summary}
\label{tab:alu_results}
\begin{tabular}{@{}lcc@{}}
\toprule
Test Category & Tests Run & Result \\
\midrule
Basic Operations & 8 & PASS \\
Edge Cases & 5 & PASS \\
Random Tests & 100 & PASS \\
Overflow Tests & 3 & PASS \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Test Analysis}

\textbf{Test Coverage:} All arithmetic and logical operations (ADD, SUB, AND, OR, XOR, shifts) were tested with:
\begin{itemize}
    \item Known test vectors
    \item Edge cases (zero inputs, maximum values)
    \item Random input combinations
    \item Overflow conditions
\end{itemize}

\textbf{No Bugs Detected:} The ALU implementation passed all tests, indicating correct functionality for the tested operations. The design properly handles:
\begin{itemize}
    \item 32-bit arithmetic with overflow wrapping
    \item Logical operations with correct bit manipulation
    \item Shift operations within specified bounds
\end{itemize}

\section{Conclusions}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{RTL Simulation:} Both Verilator and Bluesim showed equivalent performance in this experimental setup, with simulation speeds ranging from 4.50e5 to 1.25e6 cycles/second
    \item \textbf{O3 Configuration:} Larger execution resources (ROB, issue width, LSQ) provide modest CPI improvements, with Large configuration achieving the best CPI of 4.262
    \item \textbf{Simulation Trade-offs:} GEM5 offers higher simulation speed but less timing precision compared to RTL
    \item \textbf{Testing Framework:} COCOTB enables comprehensive hardware verification with Python-based testbenches
\end{enumerate}

\subsection{Design Insights}

The experiments demonstrate the importance of:
\begin{itemize}
    \item Choosing appropriate simulation tools based on accuracy vs. speed requirements
    \item Proper sizing of microarchitectural resources for performance optimization
    \item Comprehensive testing methodologies for hardware verification
\end{itemize}

\section{Future Work}

Potential extensions include:
\begin{itemize}
    \item Power consumption analysis using additional GEM5 models
    \item Custom instruction set extensions and their performance impact
    \item More complex ALU operations (multiplication, division)
    \item Cache hierarchy optimization studies
\end{itemize}

\end{document}